{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 神经网络语言模型（NNLM）\n",
        "NNLM在2003年被提出，那时的语言模型主要是以N-gram为代表的统计学模型。语言模型的任务是对一段连续的单词也就是语句的联合概率进行建模，根据的是以下的公式：\n",
        "$$P(w_1,\\cdots,w_T)=P(w_T)P(w_T|w_1,\\cdots,w_{T-1})$$\n",
        "统计学的方法是通过统计$P(w_1,\\cdots,w_T)$和$P(w_T)$来求出条件概率，但是由于$w_1,\\cdots,w_T$可以有无限的长度和庞大的词语选择空间，统计它们是非常困难的，而且有限的数据集本身也只能覆盖到所有可能句子中的非常小一部分，对于数据集中没有出现的部分不具备泛化能力。\n",
        "因此人们把能够影响一个单词概率的范围限制在了它之前的固定有限个单词，也就是有公式\n",
        "$$P(w_{T-n+1},\\cdots,w_T)=P(w_T)P(w_T|w_{T-n+1},\\cdots,w_{T-1})$$\n",
        "第T个单词的概率仅受前面的固定几个单词影响，和前面的再多的单词也没有关系了。这样做虽然减少了统计上的压力，但是也使得计算出来的条件概率不再精确。\n",
        "\n",
        "NNLM能够解决统计所有单词频率难以实现的问题，它使用一个神经网络来构造一个函数$f(i,w_{T-1},\\cdots,w_1)=P(i|w_1,\\cdots,w_{T-1})$。在了解网络具体模型之前，我们先导入一下训练用的数据集：WikiText-2，这是一个没有标注数据的语料库。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "# Prepares training data, uses WikiText-2 dataset from torchtext to train a language model\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set up training dataset and tokenizer, first iterator of\n",
        "# WikiText dataset will be loaded (downloading may consume some time).\n",
        "# Then the train_iter will be converted into list-like variable supporting __getitem__ method.\n",
        "# Otherwise the dataset could only be accessed once.\n",
        "train_iter = torchtext.datasets.WikiText2(split='train')\n",
        "train_set = to_map_style_dataset(train_iter)\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "\n",
        "# Clean and filter the dataset and shrinks size of dataset \n",
        "# to 300 for a quick training if small_set=True\n",
        "def clean_dataset(dataset, small_set=False):\n",
        "    processed_dataset = [d for d in dataset if len(tokenizer(d)) > 20] # Filters out abnormal short sentences\n",
        "    if small_set:\n",
        "        processed_dataset = processed_dataset[:300]\n",
        "    return processed_dataset\n",
        "train_set = clean_dataset(train_set, small_set=True)\n",
        "\n",
        "# Build a vocabulary\n",
        "# Feeds whole train set into the vocabulary\n",
        "def yield_tokens(dataset):\n",
        "    for text in dataset:\n",
        "        yield tokenizer(text)\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_set), specials=\"<unk>\", min_freq=3)\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "print(\"Counted %d words from train dataset\"%(len(vocab)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counted 2169 words from train dataset\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "# Build a n-gram dataloader\n",
        "# the batch from dataloader contains two tensor:\n",
        "# a context tensor with shape[batch_size, num_order-1], with each row a continous n-1 word indexes\n",
        "# a target tensor with shape[batch_size], \n",
        "# with each element one word index correspondent to its previous n-1 words\n",
        "num_order = 6 # the 'n' in n-gram\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "def build_ngram_dataset(dataset, num_order):\n",
        "    ngram_set = []\n",
        "\n",
        "    for text in dataset:\n",
        "        tokens = tokenizer(text)\n",
        "        indexes = vocab(tokens)\n",
        "        len_text = len(indexes)\n",
        "        for i in range(len_text - num_order):\n",
        "            input_tensor = torch.tensor(indexes[i:i+num_order-1], dtype=torch.int64)\n",
        "            target_tensor = torch.tensor(indexes[i+num_order], dtype=torch.int64)\n",
        "            # We don't expect model to learn to predict \"<unk>\" token\n",
        "            if vocab[\"<unk>\"] not in target_tensor:\n",
        "                ngram_set.append((input_tensor, target_tensor))\n",
        "    return ngram_set\n",
        "\n",
        "\n",
        "ngram_set = build_ngram_dataset(train_set, num_order)\n",
        "train_loader = DataLoader(ngram_set, batch_size, shuffle=True)\n",
        "print(\"Counted %d pairs in ngram_set\"%(len(ngram_set)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counted 36001 pairs in ngram_set\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面的变量中，比较有用的变量有`train_loader`, `vocab`, `tokenizer`，它们的用法如下：\n",
        "```\n",
        ">>> for input_tensor in train_loader:\n",
        ">>>     do sth... # tensor shape: [sentence_length, ]\n",
        "```\n",
        "vocab 用法:\n",
        "```\n",
        ">>> vocab(['i', 'am', 'on', 'a', 'mat'])\n",
        "[69, 1791, 17, 13, 17093]\n",
        ">>> vocab.lookup_token(187)\n",
        "large'\n",
        "```\n",
        "tokenizer 用法:\n",
        "```\n",
        ">>> tokenizer(\"Have you eaten today?\")\n",
        "['have', 'you', 'eaten', 'today', '?']\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NNLM网络结构\n",
        "NNLM整体上来说由两个部分构成：对输入进行词嵌入————把词语的索引映射成一个几十维的实向量（词嵌入），以及输入连续n个词向量预测第n+1个词语概率的前馈网络（概率函数）。用现在的眼光来看这是一个简单的不得了的网络了，但是在2003年，这是第一个提出利用预测词语的任务来同时训练词嵌入参数和概率函数参数的文章，在此之前，这些参数都是独自分别求解或者手动设置的。而且这个简单的模型揭示了一个重要的道理————人工神经网络在大量训练数据面前可以得到超越一般模型的表现\n",
        "\n",
        "<img src=\"https://image.panwenbo.icu/blog20210714225940.png\" alt=\"截屏2021-07-14 下午10.59.35\" style=\"zoom:30%;\" />\n",
        "\n",
        "### 词嵌入部分\n",
        "词嵌入部分和现在的词嵌入方法是一样的————所有词语共享同一个词嵌入矩阵的参数，因此我们可以用这一个词嵌入模型把任意词语$w_i \\in {0,1,\\cdots,|V|-1}$ 映射到 $x_i \\in R^{[m]}$其中m代表了词向量的维数，这个过程记为$x_i = C(w_i)$，而词嵌入部分的最终输出就是\n",
        "$$x=[C(w_{t-1}),C(w_{t-2}),\\cdots, C(w_{t-n+1})] \\in R^{[m\\times(n-1)]}$$\n",
        "在NNLM中我们指定一个超参数n，就像n_gram模型一样，我们只输入固定的前n个词语来预测下一个词语。因此这个模型并不一定要使用循环神经网络。词嵌入部分当中使用的参数只有一个矩阵$C \\in R^{[|V|, m]}$，$|V|$代表了所有词语的总数，这个矩阵存储了V中每一个词语对应的m维的词向量。\n",
        "\n",
        "### 概率函数\n",
        "我们现在有了词嵌入向量$x$，为了算法的非线性性，我们需要把它再进行一次非线性变换：$x'= \\tanh(Hx+ d)$。之后我们把x'输入一个线性层（$y = Ux' + b$）后带入Softmax函数得到\n",
        "$$\n",
        "\\hat P(w_t|w_{t-1}...w_{t-n+1} )=\\frac{\\exp(y_{w_t})}{\\sum_i \\exp(y_i)}\n",
        "$$\n",
        "如果x'是一个h维的隐藏层向量，y是一个维数为|V|的向量（代表了每个可能的词语的得分），那我们就可以知道其余参数的大小尺寸：\n",
        "$$H \\in R^{[h, m(n-1)]}, d \\in R^{[h]}, U \\in R^{[|V|, h]}, b \\in R^{[|V|]}$$\n",
        "\n",
        "最终我们的每个单词的得分就是：\n",
        "$$y = b + U\\tanh(d + Hx)$$\n",
        "当然，在实际实验中我们还会添加一个直接从x到y的连接，也就是：\n",
        "$$y = b + Wx + U\\tanh(d + Hx), \\ \\ \\ \\ W \\in R^{[|V|, m]}$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "# Purest NNLM class implement\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NNLM(nn.Module):\n",
        "    \"\"\"Nueral Network Language Model in its purest form\n",
        "\n",
        "        :param vocab_size: |V|, the nums of words in vocabulary\n",
        "        :type vocab_size: int\n",
        "        :param embedded_size: m, the size of embedded vector, defaults to 100\n",
        "        :type embedded_size: int, optional\n",
        "        :param num_order: n, the numbers of input words is n - 1, defaults to 6\n",
        "        :type num_order: int, optional\n",
        "        :param hidden_size: the hidden layer size in tanh, defaults to 60\n",
        "        :type hidden_size: int, optional\n",
        "        \"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, embedded_size=100, num_order=6, hidden_size=60):\n",
        "        super().__init__()\n",
        "        self.C = nn.Parameter(torch.rand((vocab_size, embedded_size)))\n",
        "        self.H = nn.Parameter(torch.rand((hidden_size, embedded_size*(num_order-1))))\n",
        "        self.d = nn.Parameter(torch.rand((hidden_size)))\n",
        "        self.U = nn.Parameter(torch.rand((vocab_size, hidden_size)))\n",
        "        self.W = nn.Parameter(torch.rand((vocab_size, embedded_size*(num_order-1))))\n",
        "        self.b = nn.Parameter(torch.rand((vocab_size)))\n",
        "        self.softmax = nn.Softmax(dim=0) # Apply softmax on vector\n",
        "    \n",
        "    def forward(self, words):\n",
        "        \"\"\"forward function\n",
        "\n",
        "        :param words: the list of word indexes with length of num_order - 1\n",
        "        :return: probabilities of all vocab words, shape: [vocab_size]\n",
        "        \"\"\"\n",
        "        x = self.C[words] # shape: [num_order-1, embedding_size]\n",
        "        x = x.view(-1)\n",
        "        y = self.b + self.W @ x + self.U @ torch.tanh(self.d + self.H @ x)\n",
        "        return self.softmax(y)"
      ],
      "outputs": [],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "# Compat version of the same model\n",
        "class NNLM_compat(nn.Module):\n",
        "    def __init__(self, vocab_size, embedded_size=100, num_order=6, hidden_size=60):\n",
        "        super().__init__()   \n",
        "        self.embedding = nn.Embedding(vocab_size, embedded_size)\n",
        "        self.tanh_layer = nn.Linear(embedded_size*(num_order-1), hidden_size)\n",
        "        self.out = nn.Linear(embedded_size*(num_order-1) + hidden_size, vocab_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, words):\n",
        "        \"\"\"forward function\n",
        "\n",
        "        :param words: the tensor of words indexes, shape: [batch_size, num_order - 1]\n",
        "        :return: possibility of all vocab-size words, shape: [batch_size, vocab_size]\n",
        "        \"\"\"\n",
        "        x = self.embedding(words)\n",
        "        x = x.view((x.size(0), -1))\n",
        "        h = torch.tanh(self.tanh_layer(x))\n",
        "        combine = torch.cat((x, h), dim=1)\n",
        "        return self.softmax(self.out(combine))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练部分\n",
        "训练的目标是给定已知的n-gram，最大化条件概率$\\hat P(w_t|w_{t-1}, \\cdots, w_{t-n+1})$，由于我们的网络输出的就是概率值，想要使该值最大化，我们只需要在输出的$w_t$对应的概率处调用.backward(-1)即可在计算图内求出每个参数的负梯度。对计算出的负梯度使用梯度下降就可以最大化需要的输出概率，当然实际操作时还需要先对SoftMax概率取一下对数来更好的优化。当然也可以使用nn.NLLLoss，有着差不多的效果"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "# Training section, firstly prepare some variables\n",
        "num_epoch = 24\n",
        "embedded_size = 100\n",
        "hidden_size = 60\n",
        "\n",
        "# Because pure version of NNLM doesn't support mini-batch \n",
        "# gradient descent, uses NNLM_compat instead\n",
        "model = NNLM_compat(len(vocab), embedded_size, num_order, hidden_size) \n",
        "# optimizer =  torch.optim.SGD(params=model.parameters(), lr=1e-3) # setting in original paper, too slow\n",
        "optimizer =  torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
        "\n",
        "# We'll use backward funtion directly on negative log \n",
        "# probabilities so loss function is no longer needed\n",
        "# criterion = nn.NLLLoss() \n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    # running_loss sum up and average all loss during \n",
        "    # the period of one epoch\n",
        "    running_loss = 0 \n",
        "    \n",
        "    for idx, batch in enumerate(train_loader):\n",
        "        input, target = batch\n",
        "        output = model(input)\n",
        "\n",
        "        # Backward propagation\n",
        "        indexes = target.view((-1, 1))\n",
        "        optimizer.zero_grad()\n",
        "        # Uses gather() to pick out elements in different columns for different rows\n",
        "        # negative log likelihood loss does the same thing as taking\n",
        "        # probalities into CrossEntropy loss function.\n",
        "        loss = -torch.log(output.gather(1, indexes)).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(\"Epoch %d:\\t|Step %d:\\t|loss=%.3f\"%(\n",
        "        epoch + 1, idx, running_loss / batch_size\n",
        "        ))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\t|Step 140:\t|loss=3.504\n",
            "Epoch 2:\t|Step 140:\t|loss=2.790\n",
            "Epoch 3:\t|Step 140:\t|loss=2.380\n",
            "Epoch 4:\t|Step 140:\t|loss=2.048\n",
            "Epoch 5:\t|Step 140:\t|loss=1.785\n",
            "Epoch 6:\t|Step 140:\t|loss=1.586\n",
            "Epoch 7:\t|Step 140:\t|loss=1.433\n",
            "Epoch 8:\t|Step 140:\t|loss=1.314\n",
            "Epoch 9:\t|Step 140:\t|loss=1.217\n",
            "Epoch 10:\t|Step 140:\t|loss=1.137\n",
            "Epoch 11:\t|Step 140:\t|loss=1.067\n",
            "Epoch 12:\t|Step 140:\t|loss=1.008\n",
            "Epoch 13:\t|Step 140:\t|loss=0.954\n",
            "Epoch 14:\t|Step 140:\t|loss=0.906\n",
            "Epoch 15:\t|Step 140:\t|loss=0.862\n",
            "Epoch 16:\t|Step 140:\t|loss=0.821\n",
            "Epoch 17:\t|Step 140:\t|loss=0.783\n",
            "Epoch 18:\t|Step 140:\t|loss=0.748\n",
            "Epoch 19:\t|Step 140:\t|loss=0.716\n",
            "Epoch 20:\t|Step 140:\t|loss=0.684\n",
            "Epoch 21:\t|Step 140:\t|loss=0.653\n",
            "Epoch 22:\t|Step 140:\t|loss=0.625\n",
            "Epoch 23:\t|Step 140:\t|loss=0.597\n",
            "Epoch 24:\t|Step 140:\t|loss=0.572\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 测试部分\n",
        "我们挑训练数据中的一段话输入模型，将输出的概率中最大概率的词语再加入原来的句子中并再次输入模型，以此类推，让模型来续写一段话。可以发现模型记住了文本的少量固定短语，而\"the\"\",\"\".\"\"is\"等词语由于有最大的先验概，基本上充满了模型的输出。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "# Evaluate by making it produce sth from its previous produce\n",
        "start_sentence = \"The building and the surrounding park were\"\n",
        "tokens = tokenizer(start_sentence)\n",
        "def pick_last_ngram_tensor(num_order):\n",
        "    indexes = vocab(tokens[-num_order + 1:])\n",
        "    return torch.tensor(indexes, dtype=torch.int64).view((1, -1))\n",
        "\n",
        "for i in range(15):\n",
        "    input = pick_last_ngram_tensor(num_order)\n",
        "    pred = model(input)\n",
        "    tokens.append(vocab.lookup_token(pred.argmax()))\n",
        "\n",
        "print(' '.join(tokens))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the building and the surrounding park were with points that the ' , had just four of and the . is to\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "原文中还使用了Perplexity($\\frac{1}{\\hat P(w_t|w_{t-1}, \\cdots, w_{t-n+1})}$的几何平均值)困惑程度来衡量模型的表现。我们编写几段文字，代入模型计算一下它们的困惑程度。如果困惑值越低，说明这句话越符合语言模型所统计到的语言模式"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "source": [
        "text1 = \"\"\"Three people walking around the river, \n",
        "only to find some dead fishes floating all around.\"\"\"\n",
        "text2 = \"\"\"Three around river walking people the, \n",
        "only to find some dead fishes floating all around.\"\"\"\n",
        "text3 = \"\"\"Three around river walking people the, \n",
        "Tel Aviv, while Haifa gained status in suffered\"\"\"\n",
        "\n",
        "def perplexity(text: str):\n",
        "    input = build_ngram_dataset([text], 6)\n",
        "    probs = 1\n",
        "    for context, target in input:\n",
        "        probs *= model(context.view(1, -1))[0, target.item()]\n",
        "    return torch.pow(probs, -1/len(input)).item()\n",
        "\n",
        "print(\"text 1 perplexity: %.3f\" % perplexity(text1))\n",
        "print(\"text 2 perplexity: %.3f\" % perplexity(text2))\n",
        "print(\"text 3 perplexity: %.3f\" % perplexity(text3))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text 1 perplexity: 637.191\n",
            "text 2 perplexity: 1801.824\n",
            "text 3 perplexity: 7079.959\n"
          ]
        }
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('torch': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "interpreter": {
      "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}