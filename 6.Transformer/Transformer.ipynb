{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer 实现序列到序列的翻译\n",
    "\n",
    "参考代码：\n",
    "https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "\n",
    "这篇Notebook里会使用我们在model.py中定义的Seq2seqTransformer类来实现一个德语到英语的翻译，双语数据集来自Torchtext中的Multi30k dataset。首先我们先获取一下需要的vocab和tokenizer。\n",
    "可能需要在bash中运行以下内容并重启jupyter来安装Spacy的依赖。\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SRC_LAN = 'de'\n",
    "TGT_LAN = 'en'\n",
    "LAN_PAIR = (SRC_LAN, TGT_LAN)\n",
    "\n",
    "# Tokenizer\n",
    "token_transform = {}\n",
    "token_transform[SRC_LAN] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LAN] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# A generator yield all words in training dataset\n",
    "def yield_tokens(data_iter, ln):\n",
    "    for sample in data_iter:\n",
    "        yield token_transform[ln](sample[LAN_PAIR.index(ln)])\n",
    "\n",
    "# Define special tokens\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "# Build vocab by build_vocab_from_iterator()\n",
    "vocab_transform = {}\n",
    "for ln in LAN_PAIR:\n",
    "    train_set = Multi30k(split='train', language_pair=LAN_PAIR)\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_set, ln), specials=special_symbols)\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "设计出字符串转词符(Token)列表，词符(Token)列表转数字索引列表，索引连接成Tensor的三个功能后，把它们连在一起得到一个字符串到张量的处理流水线。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# Turn integer list into tensor and add SOS/EOS token\n",
    "def tensor_transform(input_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]),\n",
    "                     torch.tensor(input_ids),\n",
    "                     torch.tensor([EOS_IDX])))\n",
    "\n",
    "# Combine all transforms into a pipeline\n",
    "def sequential_transform(*transforms):\n",
    "    def func(input):\n",
    "        for transform in transforms:\n",
    "            input = transform(input)\n",
    "        return input\n",
    "    return func\n",
    "\n",
    "# Text process pipeline:\n",
    "# 1) Remove all '\\n' in the end of text\n",
    "# 2) Break text from string to List[str]\n",
    "# 3) Convert List[str] to List[int] using vocab\n",
    "# 4) Concatenate SOS_IDX, List[int], EOS_IDX into tensor\n",
    "text_pipeline = {}\n",
    "for ln in LAN_PAIR:\n",
    "    text_pipeline[ln] = sequential_transform(lambda s: s.rstrip('\\n'),\n",
    "                                             token_transform[ln],\n",
    "                                             vocab_transform[ln],\n",
    "                                             tensor_transform)\n",
    "\n",
    "# Simple test\n",
    "print(text_pipeline['en'](\"Hello every one\\n\"))                                "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([   2, 6731, 4221,   55,    3])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "接着，我们需要把字符串处理的流水线集成到Pytorch的Dataloader中，并且使用pad_sequence把一个batch中的不同长度的tensor延长并拼接。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Function to collate data samples into batch tensors\n",
    "# Used when constructing Dataloader\n",
    "def collate_batch(batch):\n",
    "    src_batch = []\n",
    "    tgt_batch = []\n",
    "    for src, tgt in batch:\n",
    "        src_batch.append(text_pipeline[SRC_LAN](src))\n",
    "        tgt_batch.append(text_pipeline[TGT_LAN](tgt))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Return dataloader instance\n",
    "def get_loader(split='train', batch_size=128):\n",
    "    dataset = Multi30k(split=split, language_pair=LAN_PAIR)\n",
    "    return DataLoader(dataset, batch_size, collate_fn=collate_batch)\n",
    "\n",
    "# Simple test\n",
    "print(\"Batch shape:\", next(iter(get_loader()))[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch shape: torch.Size([27, 128])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在有了数据了, 但是还缺少Transformer输入需要的Mask，输入Transformer对于解码和编码分别需要两种Mask，具体可以看model.ipynb中的介绍。让我们定义一下。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# Get all mask\n",
    "def create_mask(src, tgt):\n",
    "    src_len = src.size(0)\n",
    "    tgt_len = tgt.size(0)\n",
    "\n",
    "    src_mask = torch.zeros(src_len, src_len)\n",
    "    tgt_mask = (1 - torch.triu(torch.ones(tgt_len, tgt_len)).T) * -1e9\n",
    "\n",
    "    # The padding mask must be batch_first\n",
    "    src_padding_mask = (src == PAD_IDX).T\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).T\n",
    "\n",
    "    return src_mask.to(DEVICE), tgt_mask.to(DEVICE), \\\n",
    "           src_padding_mask.to(DEVICE), tgt_padding_mask.to(DEVICE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们定义一个训练一个Epoch的train函数以供食用。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def train_epoch(model: nn.Module, batch_size: int, optimizer: torch.optim.Optimizer, criterion: nn.Module):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    loader = get_loader('train', batch_size)\n",
    "    for src, tgt in loader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_in = tgt[:-1, :]\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_in)\n",
    "        output = model(src, tgt_in, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt_out.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "万事具备，可以开始训练咯。如果你现在没有显卡来支持CUDA，请不要运行这段代码，尽管这已经是一个小型Transformer，但是在CPU上运行一个Batch仍然需要6-7秒钟，训练18个Epoch需要7个小时左右。将这段代码迁移到Google Colab上后大约用了11分钟完成训练。这和Pytorch原生的Transformer速度是相当的（其实比Pytorch实现还快了不少？），可见model.py中的实现速度还是可以的。后面的内容需要基于一个已经训练好的模型来进行。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from model import Seq2seqTransformer\n",
    "from utils import Timer\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LAN])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LAN])\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 18\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2seqTransformer(src_vocab_size=SRC_VOCAB_SIZE,\n",
    "                                 tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "                                 num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                                 d_model=D_MODEL,\n",
    "                                 num_heads=NHEAD,\n",
    "                                 dim_feedforward=FFN_HID_DIM).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "Timer.Start()\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    loss = train_epoch(transformer, BATCH_SIZE, optimizer, criterion)\n",
    "    print(\"Epoch: %d\\tLoss: %.3f\\tTime: %s\" % (epoch, loss, Timer.Remain(percent=epoch/NUM_EPOCHS)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练好了以后，我们希望可以测试一下模型的表现，那么面对全新的数据，我们需要使用Greedy Decode来让模型不依赖target序列输出一段翻译。Greedy Encode本质上就是一个宽度为1的beam search，每次都把拥有最高概率的预测词语添加到Target序列作为输入。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# Compute output with greedy algorithm for all source sequence in a batch\n",
    "def greedy_decode(model: Seq2seqTransformer, src, src_padding_mask=None, max_length=50):\n",
    "    src = src.to(DEVICE)\n",
    "    if src_padding_mask is not None:\n",
    "        src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "    batch_size = src.size(1)\n",
    "    model.eval()\n",
    "\n",
    "    # Forward propagation on encoder\n",
    "    src = model.positional_encoding(model.src_embedding(src))\n",
    "    memory = model.transformer.encoder.forward(src, None, src_padding_mask)\n",
    "\n",
    "    # Initialize output sequence\n",
    "    output = torch.empty(1, batch_size).fill_(SOS_IDX).long().to(DEVICE)\n",
    "    output_states = torch.zeros(batch_size)\n",
    "\n",
    "    for i in range(max_length - 1):\n",
    "        _, out_mask, _, _ = create_mask(src, output)\n",
    "\n",
    "        # Forward propagation on decoder\n",
    "        out_embedded = model.tgt_embedding(output)\n",
    "        probs = model.transformer.decoder(out_embedded, memory, out_mask)\n",
    "        probs = model.generator(probs[-1])\n",
    "\n",
    "        # Pick out most likely next word\n",
    "        next_words = probs.argmax(dim=-1)\n",
    "        output = torch.cat((output,\n",
    "                            next_words.unsqueeze(0)),\n",
    "                           dim=0)\n",
    "        \n",
    "        # Update and Checking finishing state\n",
    "        output_states[next_words == EOS_IDX] = 1\n",
    "        if not 0 in output_states:\n",
    "            break\n",
    "    \n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "把输出的预测序列编码回字符串就可以完成翻译，在Colab上经历了54个Epoches的训练以后，这段德语对应的翻译为：\"A group of people standing in front of an igloo .\"\n",
    "\n",
    "虽然我们不一定懂德语，但是可以通过翻译软件先把英语翻译成德语后再输入回模型观察效果。比如：\n",
    "\n",
    "`\"I am very happy\"` -> `\"Ich bin sehr glücklich\"` -> `\"I very happy\"`\n",
    "\n",
    "`\"Today is a good day\"` -> `\"Heute ist ein guter Tag\"` -> `\"This day are beautiful day of beautiful\"`\n",
    "\n",
    "`\"Does this steak taste good?\"` -> `\"Schmeckt dieses Steak?\"` -> `\"We are kicking each other.\"` -> `\"Wir treten uns gegenseitig.\"` -> `\"We are making a hockey.\"`\n",
    "\n",
    "`\"I was born in china\"` -> `\"Ich wurde in China geboren\"` -> `\"I see in China are competing in China .\"`\n",
    "\n",
    "`\"Please value your time\" -> \"Bitte schätzen Sie Ihre Zeit\" -> \"She is taking pictures .\"`\n",
    "\n",
    "好吧。可能模型还是比较小。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "def translate(model: Seq2seqTransformer, text):\n",
    "    # Regard input text as batch with batch_size=1\n",
    "    src = text_pipeline[SRC_LAN](text).reshape(-1, 1)\n",
    "    pred = greedy_decode(model, src, max_length=src.size(0) + 6)\n",
    "    indexes = list(pred.cpu().view(-1).long().numpy())\n",
    "    raw_str = \" \".join(vocab_transform[TGT_LAN].lookup_tokens(indexes))\n",
    "    return raw_str.replace('<sos>', '').replace('<eos>', '')\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Army railings curled avrovulcan.com World toys Younger patrol fresco playmat way album unfinished gestures credit holds\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}