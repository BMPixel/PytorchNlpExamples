{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer 模型\n",
    "这篇notebook实现了Transformer模型中的主要方法，包括多头注意力，LayerNorm等，实现的Transformer，TransformerEncoder/Decoder，TransformerEncoder/DecoderLayer类有着和Pytorch实现差不多的参数和接口，不能保证可以混用，但是至少看起来和用起来是差不多的。代码当然也比Pytorch的实现简单了很多。这份文件和model.py的内容是一样的，只不过Notebook文件多了文字说明。让我们开始吧。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210728094709.jpg\" alt=\"v2-22a369f0f1b0d542ced248dcb215b6e8_1440w\" style=\"zoom:50%;\" />\n",
    "\n",
    "参考资料 \n",
    "\n",
    "1. https://zhuanlan.zhihu.com/p/48731949 参考了主要结构和部分代码细节\n",
    "2. https://pytorch.org/tutorials/beginner/translation_transformer.html 参考了位置编码的设计\n",
    "3. ttps://pytorch.org/tutorials/beginner/transformer_tutorial.html 参考了不多\n",
    "4. https://arxiv.org/abs/1706.03762 参考了部分细节\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 实现Transformer基础结构\n",
    "Transformer的网络的基本结构是一个多头Attention加上一个全连接网络，而这两个子层都被残差连接和LayerNorm所包裹，也就是子层的输入和输出会被加在一起并且使用LayerNorm归一化。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 SublayerWrapper（图中的Add&Norm）\n",
    "为了能够把输入和输出加在一起，整个transformer的所有结构的输出输入的形状都是一样的：（L序列长度，N批大小，`d_model`输出特征大小），在原论文实现中，`d_model=512`。输出在被加上输入之前还会使用一个Dropout处理一下。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210728095532.png\" alt=\"截屏2021-07-28 上午9.55.24\" style=\"zoom:20%;\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SublayerWrapper(nn.Module):\n",
    "    def __init__(self, sub_layer, d_model, dropout_r):\n",
    "        super(SublayerWrapper, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_r)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.sub_layer = sub_layer\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.sub_layer(*args, **kwargs)\n",
    "        return self.layernorm(args[0] + self.dropout(output))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 简单测试一下\n",
    "# 要留意一下包裹的那个模块有没有被注册到，只有模块本身是类的成员变量中或是\n",
    "# 在ModuleList或ModuleDict中并且这个容器也是类的成员变量，这个模块的Parameters()\n",
    "# 才会进入到model.parameters() 并且被优化器更新\n",
    "wrapped_linear = SublayerWrapper(nn.Linear(5, 5), 5, 0.1)\n",
    "print(wrapped_linear)\n",
    "print(wrapped_linear(torch.rand(5)).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SublayerWrapper(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layernorm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
      "  (sub_layer): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 FeedForwardNet（图中的FeedForward）\n",
    "这个FFN（FeedForwardNet）由两层全连接层组成，中间使用了relu函数和dropout。由于它一次只单独处理一个时间刻，一个batch上的一个长度为`d_model`的特征向量，这个两层的网络等于两个1x1卷积。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210728095705.png\" alt=\"截屏2021-07-28 上午9.57.00\" style=\"zoom:16%;\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    \"\"\"A two-layer relu feedforward network following a multihead attention\"\"\"\n",
    "    def __init__(self, d_model, dim_feedforward, dropout=0.1):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.fc2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(input))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 简单测试一下\n",
    "LENGTH = 5\n",
    "BATCH_SIZE = 128\n",
    "D_MODEL = 512\n",
    "ffn = FeedForwardNet(512, 2048)\n",
    "print(ffn(torch.rand(LENGTH, BATCH_SIZE, D_MODEL)).shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 128, 512])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Multihead Attention\n",
    "要实现一个多头Attention，首先要实现一个单头Self-Attention:\n",
    "$$\n",
    "\\alpha_{ij}=SoftMax(\\frac{Q_iK_j^T}{\\sqrt{d_k}})\\\\\\\\\n",
    "A_i=\\textbf{Attention}(Q_i,(K,V))= \\sum_{j=1}^{d_k} \\alpha_{ij}V_j\n",
    "$$\n",
    "这是整个Transformer中最为复杂的部分。要注意这里使用的矩阵乘法，并不是大小对上了就能得到正确的结果，还要考虑不同形式的矩阵乘法中到底发生了什么。同时`mask`和`key_padding_mask`的含义是不同的，`mask`代表了在不同的时间刻要遮住哪些key，`key_padding_mask`代表了对于不同batch中的输入序列，它们的padding字符都在哪里。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210529204909.png\" alt=\"截屏2021-05-29 下午8.49.02\" style=\"zoom:10%;\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def attention(query, key, value, mask=None, key_padding_mask=None, dropout=None):\n",
    "    \"\"\"Compute scale dot product attention for given Q, K, V\n",
    "\n",
    "    Below S is the length of query, N is the batch size and E is the feature numbers,\n",
    "    T is the length of key and value. Particularly in self attention, S=T; in source attention, S!=T.\n",
    "    Assume in any condition, length of key and value is the same.\n",
    "\n",
    "    :param query: Q tensor :math:`(S, N, E)` or `(S, N_HEAD, N, E)`\n",
    "    :param key: K tensor :math:`(T, N, E)` or `(T, N_HEAD, N, E)`\n",
    "    :param value: V tensor :math:`(T, N, E)` or `(T, N_HEAD, N, E)`\n",
    "    :param mask: Mask of QKV tensor :math:`(S, T)` or `(N, N_HEAD, S, T)`, this mask \n",
    "    will be added onto scores directly.\n",
    "    :param key_padding_mask: ByteTensor mask indicating padding tokens' place.\n",
    "    place where is one shows there is a padding tokens and will be maksed.\n",
    "    shape: :math:`(N, N_Head, 1, T)`\n",
    "    :param dropout: dropout module will be applied defaults to None\n",
    "    :return: Attention values with shape:math:`(S, N, E)` or `(*, N, E)`\n",
    "    and global align weights with shape :math:`(S, N, N)` or `(*, N, N)`\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "\n",
    "    # 3d tensor matrix multiplication in torch\n",
    "    # The last two dimension will function as normal\n",
    "    # matrix multiplication while other dimensions will\n",
    "    # act as element-wise action.\n",
    "    # In order to correctly make the mask action batch-wised, \n",
    "    # First is to turn all qkv into batch_first.\n",
    "    # After we turned qkv into (N, S/T, E),\n",
    "    # we multiply (N, S, E) and (N, E, T) will get (N, S, T),\n",
    "    # which means for all N batches, the T key scores(last dim) for all S queries(second dim)\n",
    "\n",
    "    # 1) permute batch dim to the first place (S/T, N_HEAD, N, E) to (N, N_HEAD, S/T, E).\n",
    "    query = query.transpose(0, -2)\n",
    "    key = key.transpose(0, -2)\n",
    "    value = value.transpose(0, -2)\n",
    "\n",
    "    # 2) Use batched matrix multiplication to get a score tensor(N, N_HEAD, S, T)\n",
    "    scores = query @ key.transpose(-2, -1) / math.sqrt(d_k)\n",
    "\n",
    "    # 3) Use mask to set some scores to -inf and softmax\n",
    "    # For key_padding_mask, set score to -infinity will make its weight being zero\n",
    "    # As key_padding_mask shape:(N, 1, 1, T), directly using masked_fill wil broadcast\n",
    "    # it to (N, N_HEAD, S, T) and all scores will be masked correctly\n",
    "    if key_padding_mask is not None:\n",
    "        scores = scores.masked_fill(key_padding_mask, -1e9)\n",
    "\n",
    "    # For mask, the only thing need to do is to added it onto scores directly so it\n",
    "    # will be broadcast to (N, N_HEAD, S, T), applying same sequential mask to all\n",
    "    # batches equally.\n",
    "    if mask is not None:\n",
    "        scores += mask\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights)\n",
    "    \n",
    "    # 4)Compute all weighted values and transpose\n",
    "    # Final attention shape:(N, N_HEAD, T, E), transpose() is needed.\n",
    "    return (weights @ value).transpose(0, -2), weights.transpose(0, -2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 多头Attention\n",
    "多头Attention其实只需要把最后一个维度d_model reshape到(n_head, d_k)，代入attention函数就可以自然的把几个不同的头看成是互不相干的几个部分，最后再reshape回去即可。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210603212610.png\" alt=\"截屏2021-06-03 下午9.26.02\" style=\"zoom:10%;\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"MultiHead Attention module\"\"\"\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        \"\"\"MultiHead Attention module\"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "\n",
    "        # All nhead features will be combined back to one feature with d_model dim\n",
    "        assert d_model % nhead == 0\n",
    "        self.d_k = d_model // nhead\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.att_weights = None\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.out = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, key_padding_mask=None):\n",
    "        \"\"\"Forward propagation with multihead attention\n",
    "        \n",
    "        :param query: Q tensor :math:`(L, N, E)`\n",
    "        :param key: K tensor :math:`(L, N, E)`\n",
    "        :param value: V tensor :math:`(L, N, E)`\n",
    "        :param mask: Mask of QKV tensor :math:`(L, N, E)`, places where\n",
    "        mask value is zero will not be computed, defaults to None\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Expand mask shape from (L, L) to (1, 1, L, L) for the convenience to broadcast\n",
    "            # to (N, N_HEAD, L, L) to apply on all heads equally\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            # Expand padding mask shape from (N, L) to (N, 1, 1, L) in order to euqally mask \n",
    "            # different padding tokens for the correspnding batch.\n",
    "            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(1)\n",
    "        n_qlen, n_batch, d_model = query.shape\n",
    "        n_klen = key.size(0)\n",
    "\n",
    "        # Compute nhead features from q, k and v.\n",
    "        # Transpose them into shape (L, N_HEAD, N, K) for convenience of parallelization\n",
    "        query = self.q_linear(query).view(n_qlen, n_batch, self.nhead, self.d_k).transpose(1, 2)\n",
    "        key = self.k_linear(key).view(n_klen, n_batch, self.nhead, self.d_k).transpose(1, 2)\n",
    "        value = self.v_linear(value).view(n_klen, n_batch, self.nhead, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Do attention calculation and concatenate\n",
    "        att_value, self.att_weights = attention(query, key, value, mask, key_padding_mask, self.dropout)\n",
    "        att_value = att_value.transpose(1, 2).contiguous().view(n_qlen, n_batch, d_model)\n",
    "\n",
    "        return self.out(att_value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 简单测试一下\n",
    "NHEAD = 8\n",
    "ma = MultiheadAttention(D_MODEL, NHEAD, nn.Dropout(0.1))\n",
    "q = k = v = torch.rand(LENGTH, BATCH_SIZE, D_MODEL)\n",
    "mask = (1 - torch.triu(torch.ones(LENGTH, LENGTH)).T) * -1e9\n",
    "key_padding_mask = torch.zeros((BATCH_SIZE, LENGTH))\n",
    "key_padding_mask[4, 2:] = 1 # 遮住第5个样本的最后3个key\n",
    "key_padding_mask = key_padding_mask == 1\n",
    "\n",
    "attn = ma(q, k, v, mask, key_padding_mask)\n",
    "print(\"Attn shape: \", attn.shape)\n",
    "print(\"Weight shape: \", ma.att_weights.shape)\n",
    "print(\"Mask: \\n\", mask == -1e9)\n",
    "print(\"First batch: \\n\", ma.att_weights[:, 0, 0, :])\n",
    "print(\"Fifth bath: \\n\", ma.att_weights[:, 0, 4, :])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attn shape:  torch.Size([5, 128, 512])\n",
      "Weight shape:  torch.Size([5, 8, 128, 5])\n",
      "Mask: \n",
      " tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n",
      "First batch: \n",
      " tensor([[1.1111, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5837, 0.5275, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3552, 0.3709, 0.0000, 0.0000],\n",
      "        [0.2930, 0.2671, 0.2889, 0.0000, 0.0000],\n",
      "        [0.2273, 0.2204, 0.2247, 0.2184, 0.2203]], grad_fn=<SliceBackward>)\n",
      "Fifth bath: \n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5645, 0.5466, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5623, 0.5488, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5512, 0.5599, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5705, 0.5406, 0.0000, 0.0000, 0.0000]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 组装Transformer\n",
    "我们需要先把两个子层组装成Encoder层和Decoder层（decoder是三个层），然后再把这个单层连续叠加个几层就构成了一个Encoder或者Decoder，最后再把这两个拼接再一起就是一个Pytorch中的Transformer。之所以是Pytorch中的Transformer是因为Transformer还包括了把字符编码成嵌入向量和最后的Softmax输出的过程。但是Pytorch实现的Transformer只包括Encoder和Decoder。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 TransformerEncoderLayer（图中左侧）\n",
    "把Multihead Attention子层和FFN用ADD&NORM包裹以后连在一起就是Transformer中的EncoderLayer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"One encoder layer in the transformer\"\"\"\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 nhead, \n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = SublayerWrapper(\n",
    "            MultiheadAttention(d_model, nhead, nn.Dropout(dropout)),\n",
    "            d_model=d_model,\n",
    "            dropout_r=dropout)\n",
    "\n",
    "        self.ffn = SublayerWrapper(\n",
    "            FeedForwardNet(d_model, dim_feedforward, dropout),\n",
    "            d_model=d_model,\n",
    "            dropout_r=dropout)\n",
    "    \n",
    "    # src_mask will be directly added onto attention scores.\n",
    "    # src_key_padding_mask is a ByteTensor places where True located will be masked.\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        attn = self.self_attn(src, src, src, \n",
    "                              mask=src_mask, \n",
    "                              key_padding_mask=src_key_padding_mask)\n",
    "        return self.ffn(attn)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 TransformerEncoder\n",
    "只需要把多个TransformerEncoderLayer首尾相连就构成了TransformerEncoder。 注意这里如果需要复制层需要使用`copy.deepcopy()`，并记得要使用`nn.ModuleList`盛放模块使其能被torch识别"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import copy\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask, src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            src = self.norm(src)\n",
    "        return src"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 简单测试一下\n",
    "encoder_layer = TransformerEncoderLayer(D_MODEL, NHEAD)\n",
    "encoder = TransformerEncoder(encoder_layer, num_layers=2)\n",
    "src = torch.rand(LENGTH, BATCH_SIZE, D_MODEL)\n",
    "print(\"Memory shape: \", encoder(src).shape)\n",
    "print(encoder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory shape:  torch.Size([5, 128, 512])\n",
      "TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (self_attn): SublayerWrapper(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (sub_layer): MultiheadAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn): SublayerWrapper(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (sub_layer): FeedForwardNet(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerEncoderLayer(\n",
      "      (self_attn): SublayerWrapper(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (sub_layer): MultiheadAttention(\n",
      "          (q_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (out): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn): SublayerWrapper(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (sub_layer): FeedForwardNet(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 TransformerDecoderLayer（图中右侧）\n",
    "Decoder层的结构有三个子层：\b一个Self-Attention层；一个SourceAttention层，这个层使用上一个自注意力层的输出作为Q的来源，但是把Encoder的最后一层的输出作为K和V的来源；当然还有最后的FFN。因此DecoderLayer前向传播的时候还会需要输入Encoder的最后一层的输出作为Memory。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, \n",
    "                 dim_feedforward=2048, \n",
    "                 dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.src_attn = SublayerWrapper(\n",
    "            MultiheadAttention(d_model, nhead, nn.Dropout(dropout)),\n",
    "            d_model=d_model,\n",
    "            dropout_r=dropout)\n",
    "\n",
    "        self.self_attn = SublayerWrapper(\n",
    "            MultiheadAttention(d_model, nhead, nn.Dropout(dropout)),\n",
    "            d_model=d_model,\n",
    "            dropout_r=dropout)\n",
    "\n",
    "        self.ffn = SublayerWrapper(\n",
    "            FeedForwardNet(d_model, dim_feedforward, dropout),\n",
    "            d_model=d_model,\n",
    "            dropout_r=dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, \n",
    "                tgt_key_padding_mask=None, \n",
    "                memory_key_padding_mask=None):\n",
    "        tgt = self.self_attn(tgt, tgt, tgt, \n",
    "                             mask=tgt_mask, \n",
    "                             key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        tgt = self.src_attn(tgt, memory, memory, \n",
    "                            mask=memory_mask, \n",
    "                            key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        return self.ffn(tgt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 TransformerDecoder\n",
    "Decoder和Encoder在整体结构上基本上没有区别，唯一需要注意的就是由于memory和target序列都需要参与Attention计算，`memory_mask` 和 `tgt_mask`都是同一种mask。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "    \n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, \n",
    "                tgt_key_padding_mask=None, \n",
    "                memory_key_padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                        tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                        memory_key_padding_mask=memory_key_padding_mask)\n",
    "            \n",
    "            if self.norm is not None:\n",
    "                tgt = self.norm(tgt)\n",
    "            return tgt\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 简单测试一下\n",
    "TGT_LEN = 4\n",
    "SRC_LEN = 5\n",
    "\n",
    "decoder_layer = TransformerDecoderLayer(D_MODEL, NHEAD)\n",
    "decoder = TransformerEncoder(decoder_layer, num_layers=2)\n",
    "tgt = torch.rand(TGT_LEN, BATCH_SIZE, D_MODEL)\n",
    "src = torch.rand(SRC_LEN, BATCH_SIZE, D_MODEL)\n",
    "memory = encoder(src)\n",
    "\n",
    "print(\"Memory shape: \", decoder(tgt, memory).shape)\n",
    "print(encoder)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dropout'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-90266d4c22df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSRC_LEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdecoder_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNHEAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTGT_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9a8041a25851>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout)\u001b[0m\n\u001b[1;32m      4\u001b[0m                  dropout=0.1):\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerDecoderLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         self.src_attn = SublayerWrapper(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dropout'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Transformer\n",
    "我可以把Encoder和Decoder结构连在一起就得到最后的Transformer。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer Module\"\"\"\n",
    "    def __init__(self, d_model=512, nhead=8, \n",
    "                 num_encoder_layers=6, num_decoder_layers=6, \n",
    "                 dim_feedforward=2048, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, \n",
    "                memory_mask=None, src_key_padding_mask=None, \n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        memory = self.encoder(\n",
    "            src, \n",
    "            mask=src_mask, \n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "        output = self.decoder.forward(\n",
    "            tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "            )\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 输入和输出的处理\n",
    "虽然有了Transformer结构，但是现在的Transformer结构依然只能接受`d_model`维输入，产生`d_model`维的输出，因此我们还需要设计一个词嵌入结构和预测输出的结构。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 位置编码\n",
    "我们除了直接使用词嵌入方法编码输入以外，还需要把词向量加上一个同样大小的位置向量作为最终输入Encoder的词向量。位置向量公式如下：\n",
    "$$\n",
    "P_{(pos,2i)}=\\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\\\\\\\\\n",
    "P_{(pos,2i+1)}=\\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "其中`pos`指的是单词在输入序列中的位置，而`i`表示这个值在embdding向量中的位置。下面的实现中的`den`就是$\\frac{1}{10000^{2i/d}}$，这样以来Transformer就可以区分出不同位置上的词语了。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional encoding to token embedding vector\"\"\"\n",
    "    def __init__(self, embedding_dim, dropout_r, max_len=5000):\n",
    "        \"\"\"Add positional encoding to token embedding vector\n",
    "\n",
    "        :param embedding_dim: The embedding dim of positional encoding\n",
    "        :param dropout_r: Ratio of combined embedding vector dropout\n",
    "        :param max_len: Max length positinal encoding can be generated, defaults to 5000\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(torch.arange(0, embedding_dim, 2) * (-math.log(10000)) / embedding_dim)\n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        pos_embedding = torch.zeros((max_len, embedding_dim))\n",
    "        pos_embedding[:, 0::2] = torch.sin(den * pos)\n",
    "        pos_embedding[:, 1::2] = torch.cos(den * pos)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2) # Reshape into [max_len, 1, emb_size]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_r)\n",
    "\n",
    "        # Register some variables as buffers when\n",
    "        # 1) The values don't join compute graph and don't require gradients\n",
    "        # 2) Wish model.save() could save the variables\n",
    "        # 3) Wish model.to(device) could apply on the variables\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "    \n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Word embedding but scaled\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens) * math.sqrt(self.embedding_dim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 简单测试一下\n",
    "VOCAB_SIZE = 10\n",
    "\n",
    "seq = torch.ones((LENGTH, BATCH_SIZE)).long()\n",
    "embedding = TokenEmbedding(VOCAB_SIZE, D_MODEL)\n",
    "pos_embedding = PositionalEncoding(D_MODEL, 0.1)\n",
    "\n",
    "embedded = pos_embedding(embedding(seq))\n",
    "print(\"Embedded shape: \", embedded.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedded shape:  torch.Size([5, 128, 512])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 SoftMax输出\n",
    "最后的最后，还需要一个简简单单的SoftMax输出"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(-1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.softmax(self.out(input))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 合体！\n",
    "有了每一个组件，召唤黑暗大法师（雾）的过程就简单了不少。我们把最终的Transformer类叫做Seq2seqTransformer。\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210728130636.jpg\" alt=\"5a6fa4717e79dac2b2cb8ec4e5246423\" style=\"zoom:75%;\" />"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Seq2seqTransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 d_model=512, num_heads=8, dim_feedforward=2048,\n",
    "                 dropout=0.1):\n",
    "        super(Seq2seqTransformer, self).__init__()\n",
    "        self.src_embedding = TokenEmbedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = TokenEmbedding(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model, nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout)\n",
    "        \n",
    "        self.generator = Generator(d_model, tgt_vocab_size)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask,\n",
    "                src_padding_mask, tgt_padding_mask):\n",
    "        src_embedded = self.positional_encoding(self.src_embedding(src))\n",
    "        tgt_embedded = self.positional_encoding(self.tgt_embedding(tgt))\n",
    "\n",
    "        output = self.transformer.forward(\n",
    "            src=src_embedded, tgt=tgt_embedded,\n",
    "            src_mask=src_mask, tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask)\n",
    "        \n",
    "        return self.generator(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 简单测试一下\n",
    "transformer = Seq2seqTransformer(10, 10)\n",
    "src = torch.randint(0, 10, (SRC_LEN, BATCH_SIZE)).long()\n",
    "tgt = torch.randint(0, 10, (TGT_LEN, BATCH_SIZE)).long()\n",
    "src_mask = torch.zeros((SRC_LEN, SRC_LEN))\n",
    "tgt_mask = (1 - torch.triu(torch.ones(TGT_LEN, TGT_LEN)).T) * -1e9\n",
    "src_padding_mask = torch.zeros((BATCH_SIZE, SRC_LEN))\n",
    "src_padding_mask[4, 2:] = 1 # 遮住第5个样本的最后3个key\n",
    "src_padding_mask = src_padding_mask == 1\n",
    "tgt_padding_mask = torch.zeros((BATCH_SIZE, TGT_LEN))\n",
    "\n",
    "output = transformer(src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n",
    "print(\"Output shape: \", output.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output shape:  torch.Size([4, 128, 10])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}