{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec 词嵌入\n",
    "上一篇中我们了解到可以通过在神经语言模型中加入词嵌入层的方法，在训练网络的概率函数的同时也训练出一套词嵌入参数。但是在更多的情况下，我们只想要一套优秀的词嵌入矩阵，而于此无关的复杂计算我们都希望能尽可能的减少。因此谷歌提出的Word2Vec模型（准确的说是Skip-gram方法）就可以用最快的方法在更大的语料库中训练一套含义丰富的词嵌入模型\n",
    "## 训练集的设置——skip-gram和cbow方法\n",
    "现在有一句话“The quick brown fox jumps over the lazy dog”, 我们希望能够学习里面的词语和词语之间的关系。在这里我们有两种构建输入-输出数据对的方法cbow和skip-gram\n",
    "\n",
    "这两种方法都需要使用一个滑动窗口，每次在不同的位置附近连续的选择几个词语，把它们之间的联系输入网络。设想我们现在取到了这样五个词语：“quick brown fox jumps over”，那么我们对于CBOW方法和Skip-gram方法有不同的处理方式：\n",
    "### CBOW\n",
    "对于CBOW方法，我们每次选择窗口中间的一次词语，把其余的词语作为网络的输入，我们希望网络可以预测我们选中的词语，那么我们构建的数据对就有可能是这样的`(\"quick\", \"brown\", \"jumps\", \"over\") -> (\"fox“)`\n",
    "### Skip-gram\n",
    "而当我们使用Skip-gram方法时，我们会选中一个词语，然后让网络尝试预测它的上下文词语。那么我们就会构造出这些数据对：`(\"fox\") -> (\"quick\")`,`(\"fox\") -> (\"brown\")`,`(\"fox\") -> (\"jumps\")`,`(\"fox\") -> (\"over\")`\n",
    "\n",
    "事实证明，这两种方法都可以促使网络学习到一个不错的词嵌入向量。实践时，我们会利用到目标词和上下文词语的词向量，这时我们会把CBOW中所有上下文词语的词向量求平均值后再输入网络，这样CBOW和Skip-gram都是单个输入对应单个输出的任务了，我们可以用完全一样的网络在这两种方法构造的数据集上训练。而由于CBOW把上下文词向量求了平均值，可以看作是对词语分布的平滑处理，CBOW更适合于那些数据分布本来就比较不稳定的规模较小的数据集上训练，而Skip-gram则更适合于大的数据集\n",
    "\n",
    "这两种采样方法的实现在loader.py里面，下面我们就直接调用里面的函数，由于数据集比较小，之后的内容中会使用CBOW来进行训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from loader import get_dataloader\n",
    "cbow_dataloader = get_dataloader(\"cbow\", batch_size = 128, window_size=5, lines_limit=10000)\n",
    "skip_dataloader = get_dataloader(\"skip\", batch_size = 128, window_size=5, lines_limit=10000)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing vocabulary...\n",
      "Loading Yahoo dataset...\n",
      "Counted 49734 words in  dataset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec模型实现\n",
    "接下来我们只需要设计一个可以接受一个或多个单词输入，预测一个单词输出的网络模型就可以了。当然可以直接用神经语言模型来做这件事，但是这个方案有两个问题：\n",
    "\n",
    "1. 既然网络的唯一目的就是训练一套词嵌入参数，那么我们就要尽可能简化网络，只保留最关键的部分，而神经语言模型还是太复杂了\n",
    "2. 大的词嵌入模型需要为$10^5 \\sim 10^7$词语设计向量，在这么大的词语规模下，我们怎么选中那个要预测的词语？用SoftMax输出字典中每一个词语的概率吗？显然不现实。因此我们需要更好的方法。\n",
    "\n",
    "这里有三种模型可供选择，直接用SoftMax的模型，使用Hierarchical SoftMax的模型，使用Negative Sampling负采样的模型\n",
    "### 使用SoftMax的模型\n",
    "这个模型解决了网络不必要的复杂的问题但是没有解决SoftMax速度慢的问题，它的逻辑很简单：我们语料库中的每一个词语都对应一个词嵌入向量$v_w$和参数向量$\\theta_w$，其中词嵌入向量是我们希望能够训练出的有意义的向量，而参数向量只是网络里用来对其他嵌入向量进行变换的网络参数。我们对于任意输入词$w_I$和需要预测的输出词$w_O$，有如下定义：\n",
    "$$P(w_O|w_I) = SoftMax(\\theta_{w_O}^Tv_{w_I}) = \\frac{\\exp(\\theta_{w_O}^Tv_{w_I})}{\\sum_w^W \\exp(\\theta_{w}^Tv_{w_I})}$$\n",
    "在Skip-gram中，$w_I$就是选中的目标词语，$w_O$就是需要预测的附近的其他词语。在CBOW中，$w_O$就是需要预测的目标词语，但是用多个上下文词语的嵌入向量的平均值作为$v_{w_I}$\n",
    "可以看到，要最大化$P(w_O|w_I)$，不仅要算出$\\exp(\\theta_{w_O}^Tv_{w_I})$，还要算出$\\sum_w^W \\exp(\\theta_{w}^Tv_{w_I})$，这计算量实在是太大了"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Train on a very small dataset with SoftMax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import loader\n",
    "from loader import initVocab, get_dataloader\n",
    "from model import Word2VecSM\n",
    "from utils import Timer\n",
    "\n",
    "# Each time lines_limit changes, vocab need to be reload\n",
    "initVocab(lines_limit=800)\n",
    "cbow_dataloader = get_dataloader(\"cbow\", batch_size = 32, window_size=5, lines_limit=800)\n",
    "vocab_size = len(loader.vocab)\n",
    "embedding_dim = 100\n",
    "log_interval = 300\n",
    "running_loss = 0\n",
    "\n",
    "# Uses Word2Vec with SoftMax to train\n",
    "model = Word2VecSM(vocab_size, embedding_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training start with SoftMax Word2Vec\")\n",
    "for idx, batch in enumerate(cbow_dataloader):\n",
    "    input, target = batch\n",
    "    output = model(input)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if idx % log_interval == log_interval - 1:\n",
    "        print(\"Step %d:\\tloss=%.3f\\t%s\" % (idx, running_loss/log_interval, Timer.Now()))\n",
    "        running_loss = 0\n",
    "print(\"Finish Training\")\n",
    "Timer.Stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing vocabulary...\n",
      "Loading Yahoo dataset...\n",
      "Counted 10923 words in  dataset\n",
      "Training start with SoftMax Word2Vec\n",
      "Loading Yahoo dataset...\n",
      "Step 299:\tloss=16.914\t0m:0s\n",
      "Step 599:\tloss=14.707\t0m:4s\n",
      "Step 899:\tloss=13.160\t0m:8s\n",
      "Step 1199:\tloss=12.136\t0m:13s\n",
      "Step 1499:\tloss=11.339\t0m:17s\n",
      "Step 1799:\tloss=10.777\t0m:22s\n",
      "Step 2099:\tloss=10.215\t0m:26s\n",
      "Step 2399:\tloss=9.639\t0m:31s\n",
      "Finish Training\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用Hierarchical SoftMax的模型\n",
    "这是一种使用二叉树结构减少计算概率的计算量的方法：\n",
    "\n",
    "![1*dI7tiRsPrwYfw4zcukRHog](https://image.panwenbo.icu/blog20210716211151.png)\n",
    "\n",
    "如上图所示，这棵二叉树中的叶子就是一个词汇表里的单词，每个非叶节点都是一个小逻辑回归网络，这些网络可以接受一个上下文词语$w_I$输入，输出一个概率，表示如果这个上下文词语可以映射到一个目标词语$w_O$(就像我们的数据集里定义的那样),那么这个词语$w_O$出现在其右子树中的概率。设某一个节点的参数为$\\theta_k$，那么我们就有：\n",
    "$$P(w_O \\text{ in right subtree given context } w_I) = \\sigma(\\theta_k^T v_{w_I})$$\n",
    "$$P(w_O \\text{ in left subtree given context } w_I) = 1 - \\sigma(\\theta_k^T v_{w_I})$$\n",
    "这个$w_O$可以随意指定，我们可以计算任选的一个$w_O$是上下文词语$w_I$对应词语的概率。如果我们想要计算某一个$w_O$是上下文词语$w_I$对应词语的概率，我们只需要首先找出从根节点到$w_O$的唯一路径，把$w_I$带入路径中每一个节点的网络，把它们得到的概率或连续累乘，或变成1-P后再乘进去（如果这里需要向左子树前进的话），最终我们得到的乘积就是我们需要的概率。可以证明，对于同一个$w_I$，用这种方式得到的所有$P(w_O|w_I)$之和为1.\n",
    "Hierarchical SoftMax有以下特点：\n",
    "\n",
    "1. 这棵树如果有V个叶子，他就有V-1个节点和V-1个$\\theta_k$作为参数。而直接使用SoftMax分类则需要V个$\\theta_w$参数，总量上是差不多的。但是可知现在计算一个单词的概率需要的计算次数平均不超过$\\log_2 V$次， 速度大大加快了。\n",
    "2. 为了使得每个词语到根节点的距离尽可能小，可以使用一个平衡二叉树来减小树的高度；如果我们希望被频繁访问的词语的深度更小，还可以构建一个哈夫曼树来根据词语频率调节不同叶子深度。当然这里的实现两种方法都没有采用（实现起来太复杂了），采用的是直接构建一个完全二叉树，因为这样可以用一个数组来存下所有节点网络。\n",
    "3. 如果想用这个网络计算最高概率的词汇，那可没有偷懒的余地了——每次都沿着概率最高的那个子树前进并不能得到什么有意义的结果，还是需要计算所有的节点概率，考虑每个叶子节点。\n",
    "4. 可以看到，计算不同的词语需要在树中走过不同的路径，于是想要在这个网络中使用mini-batch需要保证batch里的$w_O$都是一样的，因为这个原因，在最后的实现中也不会使用Hierarchical SoftMax方法。\n",
    "\n",
    "最后得$P(w_O|w_i)$以后，我们使用交叉熵损失函数计算损失即可：\n",
    "$L = -\\log(P(w_O|w_I;\\theta))$ 但是在这里使用完全二叉树的一个问题是部分概率会由于大量连乘精度下溢变成0，于是实践中我用路径上多个负对数Sigmoid概率连加代替了连乘来防止溢出。\n",
    "\n",
    "下面的内容是使用层级SoftMax方法进行的简单的训练，有意思的是，Hierarchical SoftMax显著的慢于普通SoftMax，而且收敛的速度也非常慢。我认为的原因有两点：\n",
    "\n",
    "1. 这里的Hierarchical SoftMax的实现在batch方向上是用了for循环来做，因为不同的$w_O$的计算路径是不一样的，不易向量化，而普通SoftMax的输出也就是10000左右多分类，还没到严重拖慢算法的程度。\n",
    "2. Hierarchical SoftMax是一个多层网络，这10000多个叶子带来的树深度大约有13层，并且相对于普通SoftMax每次可以训练到所有参数，它的迭代每次只能训练很小一部分参数，这也导致收敛速度变慢。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train on a very small dataset with Hierarchical SoftMax\n",
    "import loader\n",
    "from loader import initVocab, get_dataloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Word2VecHSM\n",
    "from utils import Timer\n",
    "\n",
    "# Each time lines_limit changes, vocab need to be reload\n",
    "initVocab(lines_limit=800)\n",
    "cbow_dataloader = get_dataloader(\"cbow\", batch_size = 32, window_size=5, lines_limit=800)\n",
    "vocab_size = len(loader.vocab)\n",
    "embedding_dim = 100\n",
    "log_interval = 300\n",
    "running_loss = 0\n",
    "\n",
    "# Train with Word2Vec with Hierachical SoftMax\n",
    "# There is no need to set up a loss function because the model directly returns the sum of loss.\n",
    "model = Word2VecHSM(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training start with Hierarchical SoftMax\")\n",
    "for idx, batch in enumerate(cbow_dataloader):\n",
    "    input, target = batch\n",
    "    loss = model(input, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if idx % log_interval == log_interval - 1:\n",
    "        print(\"Step %d:\\tloss=%.3f\\t%s\" % (idx, running_loss/log_interval, Timer.Now()))\n",
    "        running_loss = 0\n",
    "print(\"Finish Training\")\n",
    "Timer.Stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Negative smapling 负采样方法\n",
    "优化SoftMax的实质是增大正确的预测的概率，缩小错误的预测的概率，而负采样方法正是显式的把这个过程拿上台面，变成了一个二分类问题。我们仅计算$P(w_O|w_I)=\\sigma(\\theta_{w_O}^Tv_{w_I})$，并希望它尽可能的大，但是为了防止网络退化成一个无论任何输入都输出1的网络，我们再对于每个$w_I$，从语料库中随机采样出k个词语$w_R$，这些词语由于是从茫茫词海中随机选到的几个词语，这k个词语几乎和$w_I$不会有任何关联，因此我们希望$P(w_R|w_I)=\\sigma(\\theta_{w_R}^Tv_{w_I})$可以尽可能的小。\n",
    "\n",
    "实践显示，在小的训练集中，k取5到20之间是比较合适的，而在大的训练集中，k取到2到5之间也是很好的。\n",
    "\n",
    "#### 如何采样$w_R$\n",
    "\n",
    "这里有两种采样方式，均匀采样（Uniform Sampling）和按频率采样（或者叫一元模型采样Unigram Sampling， $w_R \\sim U(w)$），经过研究显示，U(w)的3/4次方可以既考虑到高频词汇的重要性又兼顾到低频词汇，可以达到最好的表现，于是最佳的设置就是$w_R \\sim \\frac{U(w)}{Z}$，其中除以Z是为了归一化分布。\n",
    "\n",
    "#### 如何采样$w_I$\n",
    "\n",
    "这个问题不仅适用于负采样，也是SoftMax等方法需要考虑的。如果随机地在语料库中大量采样，那么“the”，“a“等词语可能会被处理上百万次，但是我们可能更希望多处理一些有意义的词语对比如（Paris,France）而不是（The,France）。于是我们使用以下公式进行权衡：\n",
    "$$P(w_I) = 1 - \\sqrt{\\frac{t}{f(w_I)}}$$\n",
    "更多的细节建议参考原论文，下面的实现使用最简单的从语料库中随机采样$w_I$，从词语库中均匀采样$w_R$。训练使用Skip-gram获取词语对。下面我们训练一个简单的词嵌入算法，这个算法我们就随便训练意思一下，可能是由于数据集过小的原因，事实上，训练出的词向量并不能有效的支持我们展开下面的实验。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Experiment with Negative sampling, skip-gram.\n",
    "# Train on a slightly larger dataset, but still small\n",
    "import loader\n",
    "from loader import initVocab, get_dataloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Word2VecNSLoss\n",
    "from utils import Timer\n",
    "\n",
    "# Each time lines_limit changes, vocab need to be reload\n",
    "initVocab(lines_limit=5000)\n",
    "vocab_size = len(loader.vocab)\n",
    "embedding_dim = 50\n",
    "batch_size = 128\n",
    "log_interval = 60\n",
    "\n",
    "num_epoches = 1\n",
    "K = 2 # Number of negative samples\n",
    "\n",
    "# Train with Word2Vec with Word2Vec with Negative Sampling\n",
    "# There is no need to set up a loss function because the model directly returns the average loss.\n",
    "model = Word2VecNSLoss(vocab_size, embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "print(\"Training start with Negative Sampling\")\n",
    "Timer.Start()\n",
    "for epoch in range(num_epoches):\n",
    "    # dataloader cannot be reused, must be reset each epoch.\n",
    "    running_loss = 0\n",
    "    skip_dataloader = get_dataloader(\"skip\", batch_size = 128, window_size=5, lines_limit=50)\n",
    "    for idx, batch in enumerate(skip_dataloader):\n",
    "        input, pos_context = batch\n",
    "        input = input.view((-1, 1))\n",
    "        pos_context = pos_context.view((-1, 1)) # (Batch_size, 1)\n",
    "\n",
    "        # Don't use K * batch_size, because dataloader can't guarantee the size of batch.\n",
    "        neg_context = torch.randint(vocab_size, (K*input.size(0), 1))\n",
    "\n",
    "        loss = model(input, pos_context, neg_context)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if idx % log_interval == log_interval - 1:\n",
    "            print(\"Epoch:%d \\tStep: %d\\tloss=%.3f\\t%s\" % (\n",
    "                epoch + 1, \n",
    "                idx, \n",
    "                running_loss/log_interval, \n",
    "                Timer.Remain(percent=epoch/num_epoches)))\n",
    "            running_loss = 0\n",
    "print(\"Finish Training\")\n",
    "Timer.Stop()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing vocabulary...\n",
      "Loading Yahoo dataset...\n",
      "Counted 32067 words in  dataset\n",
      "Training start with Negative Sampling\n",
      "Loading Yahoo dataset...\n",
      "Finish Training\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可视化词向量\n",
    "接下来进行几个单词位置的可视化，根据给出的几个单词进行PCA处理，取出方差最大的两个方向，显示它们的坐标。可以看出，词嵌入向量之间的平行四边形关系和聚类特点很明显的表现出来了。首都对国家成平行四边形关系，东方国家和西方国家之间有聚类的特点。不过首先我们需要先导入一个已经训练好的Glove词嵌入算法数据。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "model = torch.load(\"word2vec.model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turns a string into a embedding vector\n",
    "word2idx = lambda x: loader.vocab[loader.tokenizer(x)[0]]\n",
    "idx2vec = lambda x: model.input_embedding(torch.tensor(x).long().view(1, 1))\n",
    "word2vec = lambda x: idx2vec(word2idx(x))\n",
    "\n",
    "# Pays attention to whether these words appear in vocabulary first!\n",
    "words = [\"berlin\", \"germany\",\n",
    "         \"paris\", \"france\",\n",
    "         \"shanghai\", \"china\",\n",
    "         \"london\", \"britain\",\n",
    "         \"seoul\", \"korea\",\n",
    "         \"tokyo\", \"japan\"]\n",
    "embeddeds = [word2vec(w) for w in words]\n",
    "X = torch.cat(embeddeds, dim=0).detach().numpy()\n",
    "\n",
    "# Uses PCA method to narrow the dimensions of embedding vector down\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_new = pca.transform(X)\n",
    "x = X_new[:, 0]\n",
    "y = X_new[:, 1]\n",
    "\n",
    "# Plot and annotation\n",
    "plt.scatter(x, y,marker='o')\n",
    "for i in range(len(x)):\n",
    "    plt.annotate(words[i], xy = (x[i], y[i]), xytext = (x[i]+0.1, y[i]+0.1))\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 寻找对应的词向量\n",
    "这个实验是在所有的词向量中寻找“男孩”相对于“女孩”相当于“国王”相对于“女王”的关系。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from tqdm import tqdm # Displays progress bar\n",
    "from loader import vocab\n",
    "\n",
    "input_word1 = \"go\"\n",
    "input_word2 = \"went\"\n",
    "output_word1 = \"think\"\n",
    "output_word2 = \"<unk>\"\n",
    "output_idx = 0\n",
    "\n",
    "# Checks whether all words appears in vocabulary\n",
    "for w in [input_word1, input_word2, output_word1]:\n",
    "    if vocab[w] == vocab[\"<unk>\"]:\n",
    "        print(\"The word %s doesn't appear in vocabulary\" % w)\n",
    "\n",
    "# Finds a output_word2 that is closest to input_word2 - input_word1 + output_word1\n",
    "target_tensor = word2vec(input_word2) - word2vec(input_word1) + word2vec(output_word1)\n",
    "for i in tqdm(range(vocab_size)):\n",
    "    if not vocab.lookup_token(i) == input_word2 and\\\n",
    "       not vocab.lookup_token(i) == output_word1 and\\\n",
    "       torch.abs(idx2vec(i)-target_tensor).sum() < torch.abs(idx2vec(output_idx)-target_tensor).sum():\n",
    "        output_idx = i\n",
    "        output_word2 = vocab.lookup_token(i)\n",
    "\n",
    "print(\"\\n%s to %s is %s to (%s)\" % (input_word1, input_word2, output_word1, output_word2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32067/32067 [00:05<00:00, 6381.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "go to went is think to (thought)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}