{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TextRNN\n",
    "RNN的基本公式如下：\n",
    "$$h_t = \\tanh(W_{ih}x_x + W_{hh}h_{t-1} + b_h)$$\n",
    "只需按照顺序计算$h_1,h_2,\\cdots,h_T$即可。\n",
    "当然我们时常需要在某一些时间刻输出一些预测值，比如输出一个多分类，那么我们还会对某一个时间刻的隐藏层再单独进行一次变换：\n",
    "$$\\hat y_t = SoftMax(W_o h_t + b_o)$$\n",
    "接下来我们先导入以下这次需要使用的数据集————华尔街日报词性标注数据集，代码在loader.py中\n",
    "\n",
    "解释一下这里的词性标注的规范：这里主要会出现的几种词性有NP（名词短语），VP（动词短语），PP（介词短语），ADVP（副词短语），ADJP（形容词短语），O（句子结束符号），PAD（占位符\"<pad\\>\"对应的标注）.\n",
    "除PAD和O外，剩下的标注都以连续几个单词构成的组块出现，比如(The big car --> B-NP, I-NP, I-NP)其中B前缀表示这个位置是这个名词短语中的第一个位置，I前缀表示这个位置是在该组块的中间或末尾。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from loader import get_dataloader, print_format\n",
    "\n",
    "batch_size = 64\n",
    "length_input = 30\n",
    "\n",
    "train_loader = get_dataloader(mode=\"train\", batch_size=batch_size, sentence_length=length_input)\n",
    "dev_loader = get_dataloader(mode=\"test\", batch_size=batch_size, sentence_length=length_input)\n",
    "\n",
    "# Prints one line to visualize our dataset\n",
    "text, tags = next(iter(dev_loader))\n",
    "print_format(text[:,0], tags[:,0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset...\n",
      "Counted 19124 words in dataset \n",
      " Counted 23 output classes\n",
      " An /executive/model/would/significantly/boost/Jaguar/ 's /yearly/output/ of /50,000/cars/.\n",
      "B-NP ---I-NP-- -I-NP -B-VP -----I-VP---- -I-VP -B-NP- B-NP -I-NP- -I-NP- B-PP -B-NP- I-NP O\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 基础循环神经网络RNN\n",
    "接下来我们使用最为基础的循环神经网络来尝试解决这个问题，我们的网络的每一个时间刻的公式如下：\n",
    "$$\n",
    "\\begin{align}\n",
    "x_t &= \\text{WordEmbedding}(w_t) \\\\\n",
    "h_t &= \\tanh(W_{ih}x_x + W_{hh}h_{t-1} + b_h) \\\\\n",
    "\\hat y_t &= \\text{SoftMax}(W_o h_t + b_o) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "为了加速计算我们需要使用Mini-batch，出于简单性考虑，把所有长于30的文本都砍到30，把所有短于30的文本都使用\"<pad\\>\"字符补充到30。（这些代码都体现在loader.py中）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Define simple recurrent neural network module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"A ordinary recurrent neural network model\n",
    "\n",
    "    :param vocab_size: The number of words in vocab\n",
    "    :param output_size: The number of output classes\n",
    "    :param hidden_size: The embedding dim and hidden units in rnn\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn_linear = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Inputs in, Ouputs out, with all-zeros initial hidden units\n",
    "\n",
    "        :param input: Inputs indexes with shape [sentence_length, batch_size]\n",
    "        \"\"\"\n",
    "        sentence_length, batch_size = input.shape\n",
    "        hiddens = torch.zeros((sentence_length, batch_size, self.hidden_size))\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # Iterates on every words\n",
    "        for i in range(sentence_length):\n",
    "            combine = torch.cat((embedded[i], hiddens[i-1]), dim=1)\n",
    "            hiddens[i] = torch.tanh(self.rnn_linear(combine))\n",
    "        outputs = self.out(hiddens)\n",
    "        return self.softmax(outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Training\n",
    "from train import train, evaluate\n",
    "from loader import vocab_text, vocab_tag\n",
    "rnn = RNN(len(vocab_text), len(vocab_tag), 128)\n",
    "train(rnn, train_loader, dev_loader, \n",
    "    num_epoches=15, \n",
    "    log_interval=100, \n",
    "    learning_rate=1e-3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training starts...\n",
      "Epoch: 1\tStep: 99\tLoss= 1.112\tAcc: 0.815\t0m:7s (- 2m:16s)\n",
      "Epoch: 2\tStep: 99\tLoss= 0.488\tAcc: 0.865\t0m:16s (- 2m:0s)\n",
      "Epoch: 3\tStep: 99\tLoss= 0.372\tAcc: 0.890\t0m:24s (- 1m:49s)\n",
      "Epoch: 4\tStep: 99\tLoss= 0.302\tAcc: 0.905\t0m:33s (- 1m:39s)\n",
      "Epoch: 5\tStep: 99\tLoss= 0.256\tAcc: 0.915\t0m:42s (- 1m:29s)\n",
      "Epoch: 6\tStep: 99\tLoss= 0.220\tAcc: 0.921\t0m:50s (- 1m:20s)\n",
      "Epoch: 7\tStep: 99\tLoss= 0.193\tAcc: 0.928\t0m:58s (- 1m:11s)\n",
      "Epoch: 8\tStep: 99\tLoss= 0.169\tAcc: 0.930\t1m:7s (- 1m:2s)\n",
      "Epoch: 9\tStep: 99\tLoss= 0.153\tAcc: 0.932\t1m:15s (- 0m:53s)\n",
      "Epoch: 10\tStep: 99\tLoss= 0.135\tAcc: 0.934\t1m:24s (- 0m:44s)\n",
      "Epoch: 11\tStep: 99\tLoss= 0.122\tAcc: 0.934\t1m:32s (- 0m:36s)\n",
      "Epoch: 12\tStep: 99\tLoss= 0.108\tAcc: 0.935\t1m:40s (- 0m:27s)\n",
      "Epoch: 13\tStep: 99\tLoss= 0.099\tAcc: 0.935\t1m:48s (- 0m:18s)\n",
      "Epoch: 14\tStep: 99\tLoss= 0.088\tAcc: 0.937\t1m:56s (- 0m:10s)\n",
      "Epoch: 15\tStep: 99\tLoss= 0.079\tAcc: 0.937\t2m:4s (- 0m:1s)\n",
      "Training finishes!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 长短期记忆模型LSTM\n",
    "LSTM和RNN在基本结构上是一致的，但是在每一个单元的内容上有了比较大的变化，具体的方程为：\n",
    "$$\n",
    "\\begin{align}\n",
    "x_t &= \\text{WordEmbedding}(w_t) \\\\\n",
    "\\tilde c ^{⟨t⟩}&=\\tanh(W_c[a^{⟨t-1⟩},x^{⟨t⟩}]+b_c)\\\\\n",
    "\\Gamma_u &= \\sigma(W_u[a^{⟨t-1⟩}, x^{⟨t⟩}]+b_u)\\\\\n",
    "\\Gamma_f &= \\sigma(W_f[a^{⟨t-1⟩}, x^{⟨t⟩}]+b_f)\\\\\n",
    "\\Gamma_o&=\\sigma(W_o[a^{⟨t-1⟩},x^{⟨t⟩}]+b_o)\\\\\n",
    "c^{⟨t⟩} &= \\Gamma_u \\odot\\tilde c^{⟨t⟩}+\\Gamma_f\\odot c^{⟨t-1⟩}\\\\\n",
    "a^{⟨t⟩} &= \\Gamma_o \\odot \\tanh(c^{⟨t⟩})\\\\\n",
    "\\hat y_t &= \\text{SoftMax}(W_o a^{⟨t⟩} + b_o) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "接下来开始实现它（当然你可以直接使用nn.LSTM结束战斗）,之后我们使用同样的配置进行训练，可以看到相对于原来的RNN基本没有提升(?_?)，我认为可能的原因是词性标注任务并不是很需要长期记忆的帮助。\n",
    "\n",
    "并且进一步实验发现，Pytorch的实现nn.LSTM比我们的实现快了三倍，这到底差在哪里呢？经过一番考虑，发现把计算embedding和最后的计算output的部分都移出循环，使用向量化的方法计算（原先是在每个循环中单独计算第t时间刻的embedded和outputs, 现在的版本是全部改为向量化操作了）就可以达到和官方实现差不多的速度，可见实现循环网络的时候一定要尽可能减少在循环体中的计算而多多利用向量化计算，速度上的变化是极为可观的。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Define Long Short Term Memory Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.forget_gate = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.cell_update = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        sentence_length, batch_size = input.shape\n",
    "        hiddens = torch.zeros((sentence_length, batch_size, self.hidden_size))\n",
    "        cell_state = torch.zeros((batch_size, self.hidden_size))\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            combine = torch.cat((embedded[i], hiddens[i-1]), dim=1)\n",
    "            fg = torch.sigmoid(self.forget_gate(combine))\n",
    "            ig = torch.sigmoid(self.input_gate(combine))\n",
    "            og = torch.sigmoid(self.output_gate(combine))\n",
    "            updated_cell = torch.sigmoid(self.cell_update(combine))\n",
    "\n",
    "            cell_state = fg * cell_state + ig * updated_cell\n",
    "            hiddens[i] = og * torch.sigmoid(cell_state)\n",
    "\n",
    "        outputs = self.out(hiddens)\n",
    "        outputs = self.softmax(outputs)\n",
    "\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Training\n",
    "from train import train, evaluate\n",
    "from loader import vocab_text, vocab_tag\n",
    "\n",
    "lstm = LSTM(len(vocab_text), len(vocab_tag), 128)\n",
    "train(lstm, train_loader, dev_loader, \n",
    "    num_epoches=14, \n",
    "    log_interval=100, \n",
    "    learning_rate=1e-3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training starts...\n",
      "Epoch: 1\tStep: 99\tLoss= 1.443\tAcc: 0.748\t0m:10s (- 2m:56s)\n",
      "Epoch: 2\tStep: 99\tLoss= 0.617\tAcc: 0.859\t0m:22s (- 2m:33s)\n",
      "Epoch: 3\tStep: 99\tLoss= 0.426\tAcc: 0.885\t0m:34s (- 2m:18s)\n",
      "Epoch: 4\tStep: 99\tLoss= 0.347\tAcc: 0.899\t0m:46s (- 2m:5s)\n",
      "Epoch: 5\tStep: 99\tLoss= 0.297\tAcc: 0.908\t0m:58s (- 1m:52s)\n",
      "Epoch: 6\tStep: 99\tLoss= 0.260\tAcc: 0.917\t1m:10s (- 1m:39s)\n",
      "Epoch: 7\tStep: 99\tLoss= 0.234\tAcc: 0.922\t1m:22s (- 1m:27s)\n",
      "Epoch: 8\tStep: 99\tLoss= 0.213\tAcc: 0.925\t1m:34s (- 1m:15s)\n",
      "Epoch: 9\tStep: 99\tLoss= 0.192\tAcc: 0.927\t1m:46s (- 1m:2s)\n",
      "Epoch: 10\tStep: 99\tLoss= 0.177\tAcc: 0.931\t1m:58s (- 0m:50s)\n",
      "Epoch: 11\tStep: 99\tLoss= 0.161\tAcc: 0.933\t2m:12s (- 0m:39s)\n",
      "Epoch: 12\tStep: 99\tLoss= 0.152\tAcc: 0.934\t2m:24s (- 0m:27s)\n",
      "Epoch: 13\tStep: 99\tLoss= 0.140\tAcc: 0.934\t2m:35s (- 0m:14s)\n",
      "Epoch: 14\tStep: 99\tLoss= 0.131\tAcc: 0.934\t2m:48s (- 0m:2s)\n",
      "Training finishes!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 门控循环单元模型GRU\n",
    "GRU可以说是LSTM的简化版本，LSTM有三个门控，而GRU只有两个，当然还有只有一个门控单元的如MGU，这里不过多介绍。GRU把LSTM的最为精髓的部分保留了：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gamma_r &= \\sigma(W_r[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_r)\\\\\n",
    "\\tilde c^{⟨t⟩} &= tanh(W_c[\\Gamma_r\\odot c^{⟨t-1⟩}, x^{⟨t⟩}] + b_c)\\\\\n",
    "\n",
    "\\Gamma_u &= \\sigma(W_u[c^{⟨t-1⟩}, x^{⟨t⟩}] + b_u)\\\\\n",
    "c^{⟨t⟩} &= \\Gamma_u \\odot \\tilde c^{⟨t⟩} + (1 - \\Gamma_u) \\odot c^{⟨t-1⟩}\n",
    "\\end{align}\n",
    "$$\n",
    "经过完全一样的训练，我们发现GRU还是非常香的，不仅收敛速度快了一些，而且最终的验证集准确率也是还可以。\n",
    "\n",
    "Pytorch中一个值得注意的细节：\n",
    "```\n",
    "RuntimeError: one of the variables needed for gradient computation has been modified \n",
    "by an inplace operation: [torch.FloatTensor ], which is output 0 of UnsqueezeBackward0, \n",
    "is at version 3; expected version 2 instead. Hint: enable anomaly detection to find \n",
    "the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
    "```\n",
    "这个错误常常出现于`x[i] = self.some_module(x[i])`中，而`x = self.some_module(x)`则不会报这样的错误。\n",
    "这个错误告诉我们不要轻易在forward函数中使用tensor的部分元素赋值。因为Pytorch依赖追踪forward过程中的中间变量的引用来实现自动微分，但是tensor的部分元素赋值是按值传递，如果原先这个位置上的值已经有梯度，那么赋值也将会覆盖掉这个梯度，使得反向传播的链条就此断开。但是`x = self.some_module(x)`不会出问题的原因是x的赋值只是获得的新的值的引用，而原来的x还好好的。\n",
    "\n",
    "如果确实需要覆盖数组元素，那么就要保证覆盖前的元素没有参与运算，且覆盖后的元素也不会再被覆盖。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define Gate Recurrent Unit Module\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.update_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.reset_gate = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.cell_update = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        sentence_length, batch_size = input.shape\n",
    "        state = torch.zeros((batch_size, self.hidden_size))\n",
    "        hiddens = torch.zeros((sentence_length, batch_size, self.hidden_size))\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        for i in range(sentence_length):\n",
    "            combine = torch.cat((embedded[i], state), dim=1)\n",
    "            ug = torch.sigmoid(self.update_gate(combine))\n",
    "            rg = torch.sigmoid(self.reset_gate(combine))\n",
    "            state_gated = rg * state\n",
    "            reseted_combine = torch.cat((embedded[i], state_gated), dim=1)\n",
    "            state_updated = torch.tanh(self.cell_update(reseted_combine))\n",
    "            state = ug * state_updated + (1 - ug) * state\n",
    "\n",
    "            # Vectorizes and avoid of inplace operation error meanwhile\n",
    "            hiddens[i] = state\n",
    "        outputs = self.out(hiddens)\n",
    "        return self.softmax(outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Training\n",
    "from train import train, evaluate\n",
    "from loader import vocab_text, vocab_tag\n",
    "\n",
    "gru = GRU(len(vocab_text), len(vocab_tag), 128)\n",
    "train(gru, train_loader, dev_loader, \n",
    "    num_epoches=14, \n",
    "    log_interval=100, \n",
    "    learning_rate=1e-3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training starts...\n",
      "Epoch: 1\tStep: 99\tLoss= 1.629\tAcc: 0.722\t0m:5s (- 1m:31s)\n",
      "Epoch: 2\tStep: 99\tLoss= 0.682\tAcc: 0.828\t0m:12s (- 1m:27s)\n",
      "Epoch: 3\tStep: 99\tLoss= 0.490\tAcc: 0.863\t0m:19s (- 1m:17s)\n",
      "Epoch: 4\tStep: 99\tLoss= 0.396\tAcc: 0.884\t0m:25s (- 1m:9s)\n",
      "Epoch: 5\tStep: 99\tLoss= 0.337\tAcc: 0.895\t0m:32s (- 1m:2s)\n",
      "Epoch: 6\tStep: 99\tLoss= 0.298\tAcc: 0.905\t0m:38s (- 0m:55s)\n",
      "Epoch: 7\tStep: 99\tLoss= 0.263\tAcc: 0.911\t0m:45s (- 0m:48s)\n",
      "Epoch: 8\tStep: 99\tLoss= 0.239\tAcc: 0.916\t0m:52s (- 0m:41s)\n",
      "Epoch: 9\tStep: 99\tLoss= 0.216\tAcc: 0.920\t0m:58s (- 0m:34s)\n",
      "Epoch: 10\tStep: 99\tLoss= 0.197\tAcc: 0.923\t1m:5s (- 0m:28s)\n",
      "Epoch: 11\tStep: 99\tLoss= 0.180\tAcc: 0.926\t1m:11s (- 0m:21s)\n",
      "Epoch: 12\tStep: 99\tLoss= 0.165\tAcc: 0.928\t1m:18s (- 0m:14s)\n",
      "Epoch: 13\tStep: 99\tLoss= 0.154\tAcc: 0.929\t1m:24s (- 0m:8s)\n",
      "Epoch: 14\tStep: 99\tLoss= 0.139\tAcc: 0.930\t1m:31s (- 0m:1s)\n",
      "Training finishes!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 双向LSTM（Bi-Directional LSTM）\n",
    "双向SLTM就是两个LSTM，一个从$t=1$计算到$t=T$，一个是从$t=T$计算到$t=1$。而用于输出的全连接神经网络则在任何一个时刻都接受两个方向的LSTM单元在同一个时间刻输出的隐藏层数值进行处理。由于不再涉及新的公式，这里直接使用nn.LSTM解决了。事实证明，即使是双向LSTM也不能很好的提高这个问题上的表现。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "take = type('', (nn.Module,), dict(forward = lambda self, x: x[0]))()\n",
    "take([1,2])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define model and training.\n",
    "# Ugly but short code\n",
    "from train import train, evaluate\n",
    "from loader import vocab_text, vocab_tag\n",
    "\n",
    "hidden_size = 128\n",
    "\n",
    "bi_lstm = nn.Sequential(\n",
    "    nn.Embedding(len(vocab_text), hidden_size),\n",
    "    nn.LSTM(hidden_size, hidden_size, bidirectional=True),\n",
    "    type('TakeFirst', (nn.Module,), dict(forward = lambda self, x: x[0]))(), # 什么叫做动态语言啊（后仰）\n",
    "    nn.Linear(hidden_size * 2, len(vocab_tag)),\n",
    "    nn.LogSoftmax(dim=2)\n",
    ")\n",
    "bi_lstm.__setattr__('output_size', len(vocab_tag))\n",
    "\n",
    "train(bi_lstm, train_loader, dev_loader, \n",
    "    num_epoches=14, \n",
    "    log_interval=100, \n",
    "    learning_rate=1e-3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 总结\n",
    "对于这个词性标注的问题，看来更加复杂的网络除了增加训练时间，也都没办法对训练造成什么影响。让我们输出几个验证集中的错误例子看看到底是怎样的错误是如此的顽固不化。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# Yields out mismatched text\n",
    "# Returns an generator of wrongly tagged data\n",
    "def yield_mistakes(model, data_loader):\n",
    "    for inputs, targets in data_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=2)\n",
    "        batch_size = preds.size(1)\n",
    "        for i in range(batch_size):\n",
    "            input = inputs[:, i]\n",
    "            target = targets[:, i]\n",
    "            pred = preds[:, i]\n",
    "            if target.numel() != torch.sum(pred == target):\n",
    "                yield input, target, pred\n",
    "wrong_instances = yield_mistakes(lstm, dev_loader)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# Run this repeatly to visualize some wrongly tagged pieces\n",
    "text, tags, pred = next(wrong_instances)\n",
    "print_format(text, tags, pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " A  /pact/with/ GM /may /emerge/ in / as /little/ as /two /weeks/,/according/ to /sources/close / to /the /talks/.\n",
      "B-NP I-NP B-PP B-NP B-VP -I-VP- B-PP B-NP -I-NP- I-NP I-NP -I-NP O ---B-PP-- B-PP --B-NP- B-ADJP B-PP B-NP -I-NP O\n",
      "B-NP I-NP B-PP B-NP B-VP -I-VP- B-PP B-NP -I-NP- I-NP B-NP -I-NP O ---B-PP-- B-PP --B-NP- -I-NP- B-PP B-NP -I-NP O\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "经过对验证集和训练集中的错误数据进行观察以后，我们可以得出一个比较有信服力的解释————算法对于词语在文本中的地位并不理解。比如“before”可以表示副词“I did it before”或表示介词“before it happens”。 或者数词也可以作形容词成分“10 happy kids”，或者作名词成分“score increased 10%”，这都是词语在文本中的地位不同导致的。算法实际上没能很好地从文本中提取出不同组分之间的地位关系。\n",
    "上面提到的只是语句理解方面的问题，而影响正确率的另一个问题是算法对标记规则依然并不理解，比如算法经常会把I-XP置于一个短语组块的开头，这显然是不对的，任何短语组块的标记都应该形如“B-XP”或“B-XP, I-XP, ...”。解决这个问题我们如果想等待网络自己搞明白这件事那就太花时间了，一种可行的方法是在网络的末尾加入一个CRF层，它的主要思想是Bi-LSTM只能观察不同输入之间的关系而得到一个输出标签，但是它无法观察到其他预测的标签，自然难以协调不同标签之间的前后关系。CRF层就可以监督网络输出符合规则的标签。当然由于Pytorch没有实现CRF层，这里我们就不进行实验了。"
   ],
   "metadata": {}
  }
 ]
}