{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b4bbedfadf25860b059a1c5c39307745bbec4144bd123053550521ac6995465"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TextCNN\n",
    "TextCNN就是使用CNN来处理文本的神经网络，具体来说，虽然文本的序列是一个长度为l的一维序列，但是由于每个文本会被词嵌入算法嵌入为一个固定长度K的词嵌入向量，因此可以把整段文本看作是一个只有一个通道的$l\\times k$的特征图，在这个特征图上进行($l'\\times k$)的卷积操作，（其中l‘大约可取3-5），就可以提取出文本特征图的局部信息，构成一个新的特征图c，把这个特征图输入一个全连接神经网络来得到对文本的分类"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Prepares training data, uses IMDB dataset from torchtext to train a text classification network\n",
    "import random\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "print(\"Start Loading datasets\")\n",
    "data_iter = torchtext.datasets.IMDB(split='train') # Iterator of WikiText dataset\n",
    "data_set = to_map_style_dataset(data_iter) # Array-like variable containing all strings in train set\n",
    "tokenizer = get_tokenizer('basic_english') # Converts string into list of words\n",
    "\n",
    "data_set = list(data_set)\n",
    "random.shuffle(data_set)\n",
    "data_set = data_set[:10000] # Shrink down dataset because the original one is too large\n",
    "train_set = data_set[:9000] \n",
    "dev_set = data_set[9000:]\n",
    "\n",
    "# Build a vocabulary\n",
    "# Feeds whole train set into the vocabulary\n",
    "# Words appearing less than 3 times are not included\n",
    "print(\"Building vocabulary...\")\n",
    "def yield_tokens(dataset):\n",
    "    for _, text in dataset:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data_set), specials=[\"<unk>\", \"<pad>\"], min_freq=3)\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "print(\"Counted %d words from train dataset\"%(len(vocab)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start Loading datasets\n",
      "Building vocabulary...\n",
      "Counted 25988 words from train dataset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "# Setup train_loader and dev_loader\n",
    "import torch\n",
    "\n",
    "batch_size = 50\n",
    "length_size = 300\n",
    "\n",
    "# Replace all 'neg', 'pos' tags with 0 and 1\n",
    "# Trim all text longer than length_size to length_size\n",
    "# Expand all text short than length_size to length_size with \"<pad>\"\n",
    "# Convert all text into indexes \n",
    "# The input: list of items in train_set\n",
    "def collate_batch(batch):\n",
    "    # Replace tags with 0-1 tensor\n",
    "    batch_size = len(batch)\n",
    "    classes = [1 if tag == \"pos\" else 0 for tag, _ in batch]\n",
    "    classes = torch.tensor(classes)\n",
    "\n",
    "    # turn texts into tensor(batch_size, length_size)\n",
    "    inputs = torch.zeros((batch_size, length_size))\n",
    "    for i, sample in enumerate(batch):\n",
    "            _, text = sample\n",
    "            tokens = tokenizer(text)\n",
    "            for j in range(length_size):\n",
    "                if j < len(tokens):\n",
    "                    inputs[i, j] = vocab[tokens[j]]\n",
    "                else:\n",
    "                    inputs[i, j] = vocab[\"<pad>\"]\n",
    "\n",
    "        \n",
    "    return classes, inputs.long()\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate_batch)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextCNN模型\n",
    "下面是模型示意图：\n",
    "\n",
    "<img src=\"https://image.panwenbo.icu/blog20210719103042.png\" alt=\"截屏2021-07-19 上午10.30.32\" style=\"zoom:50%;\" />\n",
    "\n",
    "首先，我们把一段文本进行词嵌入处理后得到一张$n \\times k$的原始特征图，原文中作者同时使用来重新生成的词嵌入（可训练的）和预训练的Word2Vec词嵌入模型（不可训练）来构成了一个两层的输入，我们这里不使用预训练的输入。\n",
    "\n",
    "接着这个特征图会分别进行$(h \\times k), h \\in \\{3,4,5\\}$大小的卷积操作，每个卷积操作有100个卷积核，也就是产生了300张新的特征图$(n-h+1, 1)$。接着，每个特征图会被一个全局最大池化压缩到一个值，最后我们把这一个300维的向量输入逻辑回归模型来对文本进行一个二分类。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "# Defines the TextCNN module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.activation import LogSoftmax\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network for text classification.\n",
    "\n",
    "    :param filters: The list of all convs, containing all height and output channels of convs.\n",
    "    :type filters: List of filters, e.g. [(3, 100), (4, 100), (5, 100)]\n",
    "    :param embedding_dim: The dimension of word embedding\n",
    "    :param n_classes: The number of output classes, defaults to 2\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, embedding_dim, vocab_size, n_classes=2, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_features = sum([ch for _, ch in filters]) # the total number of output conv filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Must use nn.ModuleList here, because only registered parameters can be updated.\n",
    "        # Torch searches for every member of class instance and registeres them.\n",
    "        # The parameter in list will not be detected and rigistered, ModuleList solved that.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, ch, (h, embedding_dim)) for h, ch in filters\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.out = nn.Linear(self.n_features, n_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward function\n",
    "\n",
    "        :param input: the indexes tensors of input texts with mini-batch\n",
    "        :type input: tensor(batch_size, sentence_length)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input)\n",
    "        batch_size = input.size(0)\n",
    "        embedded = embedded.unsqueeze(dim=1) # (batch_size, channel_num=1, sentence_length, embedding_dim)\n",
    "\n",
    "        # features: the list of vectors of all filter's global max-pool outputs.\n",
    "        features = []\n",
    "        for conv in self.convs:\n",
    "            feature = F.relu(conv(embedded)) # (batch_size, channel_num=3, sentence_length - h + 1, 1)\n",
    "            feature = F.max_pool2d(feature, (feature.size(2), feature.size(3)))\n",
    "            features.append(feature[:,:,0,0])\n",
    "        stacked_feature = torch.cat(features, dim=1)\n",
    "        droped_feature = self.dropout(stacked_feature)\n",
    "        output = self.out(droped_feature)\n",
    "        return self.softmax(output)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "# Runs on dev set to evaluate accuracy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate():\n",
    "    total_correct = 0\n",
    "    for batch in tqdm(dev_loader):\n",
    "        target, input = batch\n",
    "        output = model(input)\n",
    "        total_correct += torch.sum(output.argmax(dim=1) == target)\n",
    "    return total_correct / len(dev_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training on IMDB dataset\n",
    "from utils import Timer\n",
    "filters = [(3, 50), (4, 50), (5, 50)]\n",
    "num_epoches = 16\n",
    "embedding_dim = 100\n",
    "log_interval = 100\n",
    "\n",
    "model = TextCNN(filters, embedding_dim, len(vocab))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "Timer.Start(len(train_loader) * num_epoches)\n",
    "for epoch in range(num_epoches):\n",
    "    running_loss = 0\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        target, input = batch\n",
    "        output = model(input)\n",
    "        \n",
    "        # back propagation\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        Timer.Step()\n",
    "\n",
    "        # Log progress\n",
    "        running_loss += loss.item()\n",
    "        if idx % log_interval == log_interval - 1:\n",
    "            print(\"Epoch: %d\\tStep: %d\\tLoss=%.3f\\t%s\\tAcc: %.3f\" % (\n",
    "                epoch + 1,\n",
    "                idx,\n",
    "                running_loss / log_interval,\n",
    "                Timer.Remain(),\n",
    "                evaluate()\n",
    "            ))\n",
    "            running_loss = 0"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}